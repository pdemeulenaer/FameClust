
#This script is the first analytical test of FameClust method using GMM nodes

import numpy as np
import numexpr as ne
import scipy
from scipy import signal
import os
import math
import time
import random
from random import gauss
import pylab
from pylab import *
import sys
import Lecture_module  # (Module located in /home/philippe/Desktop/Discrete_models_comparaison_jtao)
import matplotlib.pyplot as plt
#from sklearn import mixture 
from matplotlib.colors import LogNorm
#from numba import jit
#import f90_module_f2py_GALEX_UBVRI
import f90_module_f2py_UBVRI  #module fortran imported throught F2PY
#import f90_module_f2py_WFC3  #module fortran imported throught F2PY
#import f90_module_f2py_UBVRIJHK  #module fortran imported throught F2PY
#from f90_module_f2py import blas_multiplication_gmm

#Simple 1 core use: time python FameClust_analytical_FAST4.py InputFameClustNEW_WFC3_PHAT 1 601 n00
#or: time python FameClust_analytical_FAST4.py InputFameClustNEW_UBVRI_Z01900_M400 1 10 n00

# ------------------------
# Declaration of functions
# ------------------------
def reddening(M1,A_lambda_filters_selected,Rv,Ebv):
 M2 = ne.evaluate("M1 + A_lambda_filters_selected * Rv*Ebv")
 return M2

'''
def probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV,Ext_min,Ext_max):
 #f90_module_f2py_WFC3.mod.Ext_minimum = 7 #Ext_min
 #f90_module_f2py_WFC3.mod.Ext_maximum = 121 #Ext_max
 f90_module_f2py_WFC3.mod.bidu1 = Ext_min
 f90_module_f2py_WFC3.mod.bidu2 = Ext_max
 f90_module_f2py_WFC3.mod.comp_number = components_number
 f90_module_f2py_WFC3.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py_WFC3.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((121,71,71,10,5))
 f90_module_f2py_WFC3.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py_WFC3.mod.proba_node_3d_all = np.zeros((71,101,121))		#proba_node_3D = np.zeros((71,71,121))
 f90_module_f2py_WFC3.mod.blas_multiplication_gmm()
 return f90_module_f2py_WFC3.mod.proba_node_3d_all, f90_module_f2py_WFC3.mod.max_proba_position_gcc
'''

def probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV,Ext_min,Ext_max):
 #f90_module_f2py_UBVRI.mod.Ext_minimum = 7 #Ext_min
 #f90_module_f2py_UBVRI.mod.Ext_maximum = 121 #Ext_max
 f90_module_f2py_UBVRI.mod.bidu1 = Ext_min
 f90_module_f2py_UBVRI.mod.bidu2 = Ext_max
 f90_module_f2py_UBVRI.mod.comp_number = components_number
 f90_module_f2py_UBVRI.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py_UBVRI.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((121,71,71,10,5))
 f90_module_f2py_UBVRI.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py_UBVRI.mod.proba_node_3d_all = np.zeros((71,101,121))		#proba_node_3D = np.zeros((71,71,121))
 f90_module_f2py_UBVRI.mod.blas_multiplication_gmm()
 return f90_module_f2py_UBVRI.mod.proba_node_3d_all, f90_module_f2py_UBVRI.mod.max_proba_position_gcc

'''
def probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV):
 f90_module_f2py_UBVRI.mod.comp_number = components_number
 f90_module_f2py_UBVRI.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py_UBVRI.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((121,71,71,10,5))
 f90_module_f2py_UBVRI.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py_UBVRI.mod.proba_node_3d_all = np.zeros((71,71,121))		#proba_node_3D = np.zeros((71,71,121))
 f90_module_f2py_UBVRI.mod.blas_multiplication_gmm()
 return f90_module_f2py_UBVRI.mod.proba_node_3d_all, f90_module_f2py_UBVRI.mod.max_proba_position_gcc

def probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV):
 f90_module_f2py_UBVRIJHK.mod.comp_number = components_number
 f90_module_f2py_UBVRIJHK.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py_UBVRIJHK.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((121,71,71,10,5))
 f90_module_f2py_UBVRIJHK.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py_UBVRIJHK.mod.proba_node_3d_all = np.zeros((71,71,121))		#proba_node_3D = np.zeros((71,71,121))
 f90_module_f2py_UBVRIJHK.mod.blas_multiplication_gmm()
 return f90_module_f2py_UBVRIJHK.mod.proba_node_3d_all, f90_module_f2py_UBVRIJHK.mod.max_proba_position_gcc

def probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV):
 f90_module_f2py_GALEX_UBVRI.mod.comp_number = components_number
 f90_module_f2py_GALEX_UBVRI.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py_GALEX_UBVRI.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((121,71,71,10,5))
 f90_module_f2py_GALEX_UBVRI.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py_GALEX_UBVRI.mod.proba_node_3d_all = np.zeros((71,71,121))		#proba_node_3D = np.zeros((71,71,121))
 f90_module_f2py_GALEX_UBVRI.mod.blas_multiplication_gmm()
 return f90_module_f2py_GALEX_UBVRI.mod.proba_node_3d_all, f90_module_f2py_GALEX_UBVRI.mod.max_proba_position_gcc
'''

#Function for boxcar smoothing (1)
def runningMean(x, N):
    y = np.zeros((len(x),))
    for ctr in range(len(x)):
         y[ctr] = np.sum(x[ctr:(ctr+N)])
    return y/N
#Function for boxcar smoothing (2)
def runningMeanFast(x, N):
    return np.convolve(x, np.ones((N,))/N)[(N-1):]

def gaussian(x, mu, sig):
    return sig**(-1) * (2*np.pi)**-0.5 * np.exp(-0.5*np.power(x-mu, 2.) * (np.power(sig, -2.)) )


#@jit
def Age_convolution_3D(proba_node_3D,sigma_conv_age):   #Convolution of a 3D array, line by line!! 
    #1D CONVOLUTION AGE
    size_kernel = 10
    array_padding_left = np.zeros((size_kernel+1,101,121))
    array_padding_right = np.zeros((size_kernel+1,101,121))
    #1. Padding by relevant values (not 0s)
    for Ext_loop in range(1,122):    
     for mm_loop in range(1,102):
      mean_left  = np.mean(proba_node_3D[0:sigma_conv_age,mm_loop-1,Ext_loop-1]) 
      mean_right = np.mean(proba_node_3D[-sigma_conv_age:,mm_loop-1,Ext_loop-1])   
      for ii in range(0,size_kernel+1):        
       x = ii        
       array_padding_left[-ii-1,mm_loop-1,Ext_loop-1] = mean_left*gaussian(x, 0, sigma_conv_age)
       array_padding_right[ii,mm_loop-1,Ext_loop-1]   = mean_right*gaussian(x, 0, sigma_conv_age)      
        
    proba_node_3D_padded = np.concatenate((array_padding_left, proba_node_3D, array_padding_right), axis=0)
    #print len(proba_node_3D_padded[:,0,0]),len(proba_node_3D_padded[0,:,0]),len(proba_node_3D_padded[0,0,:])    
    
    #2. Convolution
    space = np.arange(-size_kernel,size_kernel+1,1)
    kernel = gaussian(space, 0, sigma_conv_age)
    proba_node_3D_smooth = np.zeros((71+size_kernel+1+size_kernel+1,101,121))
    for Ext_loop in range(1,122): 
     for mm_loop in range(1,102):
      proba_node_3D_smooth[:,mm_loop-1,Ext_loop-1] = scipy.signal.convolve(proba_node_3D_padded[:,mm_loop-1,Ext_loop-1], kernel,'same')
    proba_node_3D_smooth = proba_node_3D_smooth[size_kernel+1:-(size_kernel+1),:,:] 
    #print len(proba_node_3D_smooth[:,0,0]),len(proba_node_3D_smooth[0,:,0]),len(proba_node_3D_smooth[0,0,:])    
    return proba_node_3D_smooth

#@jit    
def Mass_convolution_3D(proba_node_3D,sigma_conv_mass):
    #1D CONVOLUTION MASS
    size_kernel = 10
    array_padding_left = np.zeros((71,size_kernel+1,121))
    array_padding_right = np.zeros((71,size_kernel+1,121))
    #1. Padding by relevant values (not 0s)
    for Ext_loop in range(1,122):  
     for aa_loop in range(1,72):
      mean_left  = np.mean(proba_node_3D[aa_loop-1,0:sigma_conv_mass,Ext_loop-1]) 
      mean_right = np.mean(proba_node_3D[aa_loop-1,-sigma_conv_mass:,Ext_loop-1])   
      for ii in range(0,size_kernel+1):        
       x = ii        
       array_padding_left[aa_loop-1,-ii-1,Ext_loop-1] = mean_left*gaussian(x, 0, sigma_conv_mass)
       array_padding_right[aa_loop-1,ii,Ext_loop-1]   = mean_right*gaussian(x, 0, sigma_conv_mass)          
    proba_node_3D_padded = np.concatenate((array_padding_left, proba_node_3D, array_padding_right), axis=1)
    #print len(proba_node_3D_padded[:,0,0]),len(proba_node_3D_padded[0,:,0]),len(proba_node_3D_padded[:,0,0])     
    
    #2. Convolution
    space = np.arange(-size_kernel,size_kernel+1,1)
    kernel = gaussian(space, 0, sigma_conv_mass)
    proba_node_3D_smooth = np.zeros((71,101+size_kernel+1+size_kernel+1,121))
    for Ext_loop in range(1,122):    
     for aa_loop in range(1,72):
      proba_node_3D_smooth[aa_loop-1,:,Ext_loop-1] = scipy.signal.convolve(proba_node_3D_padded[aa_loop-1,:,Ext_loop-1], kernel,'same')
    proba_node_3D_smooth = proba_node_3D_smooth[:,size_kernel+1:-(size_kernel+1),:] 
    #print len(proba_node_3D_smooth[:,0,0]),len(proba_node_3D_smooth[0,:,0]),len(proba_node_3D_smooth[:,0,0])    
    return proba_node_3D_smooth   

#@jit    
def Ebv_convolution_3D(proba_node_3D,sigma_conv_Ebv):
    #1D CONVOLUTION EXTINCTION
    size_kernel = 10
    array_padding_left  = np.zeros((71,101,size_kernel+1))
    array_padding_right = np.zeros((71,101,size_kernel+1))
    #1. Padding by relevant values (not 0s)
    for mm_loop in range(1,102):  
     for aa_loop in range(1,72):
      mean_left  = np.mean(proba_node_3D[aa_loop-1,mm_loop-1,0:sigma_conv_Ebv]) 
      mean_right = np.mean(proba_node_3D[aa_loop-1,mm_loop-1,-sigma_conv_Ebv:])   
      for ii in range(0,size_kernel+1):        
       x = ii        
       array_padding_left[aa_loop-1,mm_loop-1,-ii-1] = mean_left*gaussian(x, 0, sigma_conv_Ebv)
       array_padding_right[aa_loop-1,mm_loop-1,ii]   = mean_right*gaussian(x, 0, sigma_conv_Ebv)          
    proba_node_3D_padded = np.concatenate((array_padding_left, proba_node_3D, array_padding_right), axis=2)
    #print len(proba_node_3D_padded[:,0,0]),len(proba_node_3D_padded[0,:,0]),len(proba_node_3D_padded[0,0,:])     
    
    #2. Convolution
    space = np.arange(-size_kernel,size_kernel+1,1)
    kernel = gaussian(space, 0, sigma_conv_Ebv)
    proba_node_3D_smooth = np.zeros((71,101,121+size_kernel+1+size_kernel+1))
    for mm_loop in range(1,102):    
     for aa_loop in range(1,72):
      proba_node_3D_smooth[aa_loop-1,mm_loop-1,:] = scipy.signal.convolve(proba_node_3D_padded[aa_loop-1,mm_loop-1,:], kernel,'same')
    proba_node_3D_smooth = proba_node_3D_smooth[:,:,size_kernel+1:-(size_kernel+1)] 
    #print len(proba_node_3D_smooth[:,0,0]),len(proba_node_3D_smooth[0,:,0]),len(proba_node_3D_smooth[0,0,:])    
    return proba_node_3D_smooth 

def Proba_map_integration(proba_node_3D,sigma_age,sigma_mass,sigma_Ebv):
 proba_node_3D_integrated = np.zeros((71,101,121))    
 proba_node_3D_padded = np.zeros((2*sigma_age+71,2*sigma_mass+101,2*sigma_Ebv+121)) 
 proba_node_3D_padded[sigma_age:-sigma_age,sigma_mass:-sigma_mass,sigma_Ebv:-sigma_Ebv] = proba_node_3D
 for Ext_loop in range(1,122):    
  for mm_loop in range(1,102):  
   for aa_loop in range(1,72):
    proba_node_3D_integrated[aa_loop-1,mm_loop-1,Ext_loop-1] = proba_node_3D_padded[sigma_age+aa_loop-1-sigma_age:sigma_age+aa_loop-1+sigma_age+1,
                                                                             sigma_mass+mm_loop-1-sigma_mass:sigma_mass+mm_loop-1+sigma_mass+1,
                                                                             sigma_Ebv+Ext_loop-1-sigma_Ebv:sigma_Ebv+Ext_loop-1+sigma_Ebv+1].sum() 
 return proba_node_3D_integrated   


#Here is the tool which allows to derive the correct A_lambda_on_Av for the desired E(B-V),age, and filters (of SSP Zn00)


'''
#Parameters
age_selected = 7.1 
Ebv_selected = 3.1
number_filters = 6
filters_selected_index = np.arange(number_filters)
filters_selected_index[0] = 11
filters_selected_index[1] = 12
filters_selected_index[2] = 13
filters_selected_index[3] = 14
filters_selected_index[4] = 15
filters_selected_index[5] = 16
'''
def Find_correct_A_lambda_on_Av(A_lambda_on_Av_all_Av,age_selected,Ebv_selected,filters_selected_index):
 Av = 3.1*Ebv_selected
 low_Av = np.int(np.floor(Av))
 up_Av  = np.int(np.ceil(Av))    
 Av_array_low_up = np.zeros(2)
 Av_array_low_up[0] = np.floor(Av)
 Av_array_low_up[1] = np.ceil(Av)
 #print Av_array_low_up, Av     
 number_filters = len(filters_selected_index)       
 #Here we select the Av for the correct age and filters 
 age_array = A_lambda_on_Av_all_Av[:,0,0]
 index_age = ne.evaluate('(age_array>=age_selected-0.001) & (age_array<=age_selected+0.001)') #Selection of good age   
 Av_array_low_up_value = np.zeros((2,number_filters))
 Av_array_low_up_value[0,:] = A_lambda_on_Av_all_Av[index_age,filters_selected_index[0]:filters_selected_index[0]+number_filters,low_Av]
 Av_array_low_up_value[1,:] = A_lambda_on_Av_all_Av[index_age,filters_selected_index[0]:filters_selected_index[0]+number_filters,up_Av]
 #print A_lambda_on_Av_all_Av[index_age,11:17,low_Av]
 #print A_lambda_on_Av_all_Av[index_age,11:17,up_Av]
 #Here we interpolate between the Av tables to get the correct values for selected Av        
 A_lambda_Av_used = np.zeros(number_filters)
 for ff in range(0,number_filters):
  A_lambda_Av_used[ff] = np.interp(Av,Av_array_low_up,Av_array_low_up_value[:,ff])    
 return A_lambda_Av_used
'''
A_lambda_Av_used = Find_correct_A_lambda_on_Av(A_lambda_on_Av_all_Av,age_selected,Ebv_selected,filters_selected_index)
print A_lambda_Av_used
'''






# -------------------------
# Loading of the input file
# -------------------------
print
print 80*'-'
print
print 50*'-'
print 'Loading of InputFile'
print 50*'-'

os.system("date")
InputFile_Name = sys.argv[1] # The name of input file is given during execution of the script
number_begin = int(sys.argv[2])
number_end = int(sys.argv[3])
number_clusters = int(sys.argv[3])
Z_indice = sys.argv[4]
Z,zz = Lecture_module.Zindex_to_Z_and_zz(Z_indice)


InputFile = open('/home/philippe/Desktop/Discrete_models_comparaison_jtao/SC_Parameters_20/'+InputFile_Name).readlines()
my_list = []
for line in InputFile:
    item = str.split(line)
    if item[0][0] != '#':
     my_list.append(item[0])

number_filters = int(my_list[0])
print 'Number of filters selected:                 ', number_filters

filters_selected_index = np.arange(number_filters) 	#integer array containing the indexes of the selected filters 
for ii in range(0,number_filters):
 filters_selected_index[ii] = int(my_list[ii+1])
print 'Indexes of filters selected:               ', filters_selected_index

Distance_modulus_host_galaxy = my_list[number_filters+1]
print 'Distance modulus of the host galaxy:        ', Distance_modulus_host_galaxy
app_or_abs = int(my_list[number_filters+2])
print 'Apparent mags [1], Absolute mags [2]:       ', app_or_abs
file_observed_clusters = my_list[number_filters+3]
print 'Input file of the observed clusters:        '
print '    ',file_observed_clusters
number_cluster_observed = int(my_list[number_filters+4])
print 'Number of observed clusters:                ',  number_cluster_observed	#[obsolete]
choice_extinction = int(my_list[number_filters+5])
print 'Cluster(s) extincted [1], not extincted [2]:', choice_extinction
choice_extinction_law = int(my_list[number_filters+6])
print 'Extinction law of MW [1], of LMC [2]:       ', choice_extinction_law
path_file_out_cluster = my_list[number_filters+7]
print 'Path of output files for derived parameters:' 
print '    ',path_file_out_cluster
print 50*'-'
print 'InputFile loaded'
print 50*'-'
print
#raw_input()


# -----------------------------------------------------------------------------------------
#Loading of the A_lambda (extinction parameters) for the filters selected in the input file
# -----------------------------------------------------------------------------------------
#PART 1: we load the fixed A_lambda/Av
choice_extinction_law = 2
print
print 50*'-'
print 'Loading of the A_lambda (extinction parameters)'
print 50*'-'
# lambda, lambda_f_MW, lambda_f_LMC, index of filter [1-->52]
#filters_A_lambda = np.genfromtxt('Filters_information.dat',comments='#')
#filters_A_lambda = np.genfromtxt('Filters_information_observations_WFC3_from_Welch.dat',comments='#')
filters_A_lambda = np.genfromtxt('Filters_information_observations_WFC3_from_STSCI.dat',comments='#')
if choice_extinction_law == 1:	#MW
 A_lambda_filters_selected = filters_A_lambda[filters_selected_index-1,1]
 Rv = 3.1
if choice_extinction_law == 2:	#LMC
 A_lambda_filters_selected = filters_A_lambda[filters_selected_index-1,2]
 Rv = 3.4
print A_lambda_filters_selected
del filters_A_lambda

#PART 2: We load all the A_lambda/Av tables in one 3D array (age,filters,Av); this is a more accurate description of the extinction
A_lambda_on_Av_all_Av = np.zeros((71,1+52,11))
for Av_index in range(0,11):
 A_lambda_on_Av_all_Av[:,:,Av_index] = np.genfromtxt('/home/philippe/Desktop/Discrete_models_comparaison_jtao/A_lambda_on_Av_grid_Zn00/A_lambda_on_Av_for_Av_{0}'.format(Av_index),comments='#')

print 50*'-'
print 'A_lambda loaded'
print 50*'-'
print
#raw_input()


# -----------------------------------------------------------------------------------------
#Loading of the observations
# -----------------------------------------------------------------------------------------
choice_sigma_observation = 0 #Artificial tests
#choice_sigma_observation = 1 #data with sigmas given
print
print 50*'-'
print 'Loading of the observation'
print 50*'-'
data_input         = np.genfromtxt(path_file_out_cluster+file_observed_clusters, comments='#')
data_input_names   = np.genfromtxt(path_file_out_cluster+file_observed_clusters, comments='#',names=True)
ID = data_input_names['ID']
observations       = np.zeros((len(data_input[:,0]),number_filters))
observations_sigma = np.zeros((len(data_input[:,0]),number_filters))
print 'Filters loaded: '
for ff in range(0,number_filters):
 name_filter, e_name_filter = Lecture_module.Filters_Index_Name(filters_selected_index[ff])
 print name_filter
 observations[:,ff]       = data_input_names[name_filter]
 if choice_sigma_observation == 1: #data with given sigmas
  observations_sigma[:,ff] = np.maximum(0.05,data_input_names[e_name_filter])
 # observations_sigma[:,ff] = data_input_names[e_name_filter]
 if choice_sigma_observation == 0: #Artificial tests with artificial sigmas
  observations_sigma[:,ff] = 0.05  #MODIFY THIS
#observations_sigma[:,5:8] = 0.10
#observations_sigma[:,0:2] = 0.15 #9e59
print 'observations: ', observations
observations[:,:] = observations[:,:] - np.float(Distance_modulus_host_galaxy)
#REJECTING GAP DATA (by increasing the sigmas of these gaps data to infinite)
observations_gaps_mask = ne.evaluate('observations>30.')
observations[observations_gaps_mask] = 99.999
observations_sigma[observations_gaps_mask] = 9e19
print 'observations: ', observations

#We filter out the sigmas of bad data: from 99.9 -> 9e59   #this works when choice_sigma_observation == 1 (when using real sigmas)
filter_sigma = ne.evaluate('observations_sigma>10.')       #this filter is based on rejecting data with very large sigmas
observations_sigma[filter_sigma] = 9e19

#observations_sigma[:,0] = observations_sigma[:,0] + 0.03
#observations_sigma[:,1] = observations_sigma[:,1] + 0.03
#observations_sigma[:,2] = observations_sigma[:,2] + 0.03
#observations_sigma[:,3] = observations_sigma[:,3] + 0.05
#observations_sigma[:,4] = observations_sigma[:,4] + 0.20
#observations_sigma[:,5] = observations_sigma[:,5] + 0.30

print 'sigmas of observations: ', observations_sigma
#age_true  = data_input_names['logt']	#If available data
#mass_true = data_input_names['logm']	#If available data
#Ebv_true  = data_input_names['Av']*3.1	#If available data
del data_input,data_input_names
print 50*'-'
print 'Observation loaded'
print 50*'-'
print
#raw_input()

Ebv_min = 0.06
Ebv_max = 1.1
Ext_min,Ext_max=np.int(Ebv_min*100+1),np.int(Ebv_max*100+1)
#print Ext_min,Ext_max


#READING THE GRID, AND STORING THE NODES
Grid_nodes_means                   = np.zeros((71,101,10,number_filters))
Grid_nodes_means_reddened          = np.zeros((121,71,101,10,number_filters))
Grid_nodes_covariances             = np.zeros((71,101,10,number_filters,number_filters))
Grid_nodes_covariances_INV         = np.zeros((71,101,10,number_filters,number_filters))
Grid_nodes_covariances_DET_inv     = np.zeros((71,101,10))
Grid_nodes_weights                 = np.zeros((71,101,10))
Grid_nodes_covariances_M           = np.zeros((71,101,10,number_filters,number_filters))
Grid_nodes_covariances_M_INV       = np.zeros((71,101,10,number_filters,number_filters))
Grid_nodes_covariances_M_DET       = np.zeros((71,101,10))
Grid_nodes_covariances_M_DET_inv   = np.zeros((71,101,10))

for mm in range(1,102):   
 print mm
 for aa in range(1,72):
  age, mass, Z, age_indice, mass_indice, Z_indice = Lecture_module.age_mass_Z_December2011(aa,mm,zz)

  #Open the npz node file   
  #path_grid = '/home/philippe/Desktop/Discrete_models_comparaison_jtao/Grid_HRS_allZ_ExpFactor6_Weidner_corrected_NPZ/NPZ_files/{0}/'.format(Z_indice) #in HDD
  #path_grid = '/opt/Grid_HRS_allZ_ExpFactor6_Weidner_corrected_NPZ/NPZ_files/{0}/'.format(Z_indice) #in SSD    (1000 models per node)  
  #path_grid = '/opt/Grid_HRS_allZ_ExpFactor6_Weidner_corrected_NPZ/NPZ_files_5000models_per_node/{0}/'.format(Z_indice) #in SSD    (1000 models per node)  
  #path_grid = '/home/philippe/Desktop/Discrete_models_comparaison_jtao/Grid_HRS_allZ_ExpFactor6_Weidner_corrected_NPZ/NPZ_files_5000models_per_node_with_ACS_PEGASE/{0}/'.format(Z_indice) #in HDD    (1000 models per node)  #CENTERED ON PEGASE SSP!!!!
  #path_grid = '/home/philippe/Desktop/Discrete_models_comparaison_jtao/Grid_FRS_allZ_Kroupa_NPZ/NPZ_files/{0}/'.format(Z_indice) #in HDD    (1000 models per node)  #CENTERED ON PADOVA SSP
  #path_grid = '/home/philippe/Desktop/Discrete_models_comparaison_jtao/Grid_FRS_allZ_Kroupa_NPZ/NPZ_files_PEGASE/{0}/'.format(Z_indice) #in HDD    (1000 models per node)  #CENTERED ON PEGASE SSP!!!!

  path_grid = '/opt/Grid_FRS_allZ_Kroupa_NPZ/NPZ_files/{0}/'.format(Z_indice) #in SSD    #CENTERED ON PADOVA SSP
  node = np.load(path_grid+'Clusters_t{0}_M{1}_Z{2}.npz'.format(age_indice,mass_indice,Z_indice))

  #Extract the means, covariances and weights
  means       			  = node['means']
  covariances 			  = node['covariances']
  weights     			  = node['weights']
  Grid_nodes_weights[aa-1,mm-1,:] = node['weights']
  components_number = len(weights)

  #Extracting them for the good filters, in grids
  for ii in range(0,components_number):
   Grid_nodes_means[aa-1,mm-1,ii,:]           = means[ii,filters_selected_index[0]-1:filters_selected_index[0]-1+number_filters]    
   Grid_nodes_covariances[aa-1,mm-1,ii,:,:]   = covariances[ii,filters_selected_index[0]-1:filters_selected_index[0]-1+number_filters,filters_selected_index[0]-1:filters_selected_index[0]-1+number_filters] 

  #REDDENING OF THE MEANS, TO ACCOUNT FOR EXTINCTED NODES
  for Ext in range(Ext_min,Ext_max+1):  #1,122 -> E(B-V)=[0,1]
   Ebv = (Ext-1)*0.01

   #HERE I INCLUDE THE SOPHISTICATION ON EXTINCTION (this is an option, comment it if not desired. 
   #If commented, value will be from the file opened in #Loading of the A_lambda#, it will be fixed values correct for Vega star only )
   #A_lambda_filters_selected = Find_correct_A_lambda_on_Av(A_lambda_on_Av_all_Av,age,Ebv,filters_selected_index)
   #print A_lambda_filters_selected
   #A_lambda_filters_selected[0] = A_lambda_filters_selected[0] - 0.002
   #A_lambda_filters_selected[1] = A_lambda_filters_selected[1] - 0.003
   #A_lambda_filters_selected[2] = A_lambda_filters_selected[2] - 0.02 
   #A_lambda_filters_selected[3] = A_lambda_filters_selected[3] - 0.015
   #A_lambda_filters_selected[4] = A_lambda_filters_selected[4] - 0.018
   #A_lambda_filters_selected[5] = A_lambda_filters_selected[5] - 0.0035

   for ii in range(0,components_number):
    Grid_nodes_means_reddened[Ext-1,aa-1,mm-1,ii,:] = reddening(Grid_nodes_means[aa-1,mm-1,ii,:],A_lambda_filters_selected,Rv,Ebv)
print 'GRID LOADED'
print 'from: ', path_grid
#print A_lambda_filters_selected
#raw_input()


#BUILDING OF THE MATRIX CONTAINING THE SIGMAS OF DATA
Grid_nodes_covariances_obs_large   = np.zeros((number_clusters,71,101,10,number_filters,number_filters))
Grid_nodes_covariances_obs         = np.zeros((number_filters,number_filters)) #This is the cov matrix of uncertainties, containing sigmas of photometric errors

#if choice_sigma_observation == 0: #For the artificial tests  !HERE BUG!!! BE CAREFUL IF WE USE AUTOMATIC SIGMAS FOR REAL DATA, WHICH HAVE GAPS!!
# for ff in range(0,number_filters): #Building the sigma covariance matrix
#  Grid_nodes_covariances_obs[ff,ff] = observations_sigma[0,ff]*observations_sigma[0,ff]  #variance, not stdev

#here we build the large matrix (containing the sigmas for all the clusters)
for list_SC in range(0,number_clusters):
 print list_SC
 #if choice_sigma_observation == 1: #For data with given sigmas
 for ff in range(0,number_filters): #Building the sigma covariance matrix
  Grid_nodes_covariances_obs[ff,ff] = observations_sigma[list_SC,ff]*observations_sigma[list_SC,ff]  #variance, not stdev
 if list_SC==0: print Grid_nodes_covariances_obs
 for mm in range(1,102):   
  for aa in range(1,72):
   for ii in range(0,components_number):
    Grid_nodes_covariances_obs_large[list_SC,aa-1,mm-1,ii,:,:] = Grid_nodes_covariances_obs[:,:]
print 'MATRIX SIGMA LOADED'  #NOTE THAT THIS TRIPLE LOOP HERE NEEDS A LOT OF RAM. I SHOULD SIMPLIFY!!!
#raw_input()




solution = np.zeros((number_clusters,8))
list_x_label_position = np.array([1., 9., 19., 29., 39., 49., 59., 69.])
list_x_label_name = 6.6+(list_x_label_position[:]-1.)*0.05
list_y_label_position = np.array([1., 11., 21., 31., 41., 51., 61., 71.])
list_y_label_name = 2.+(list_y_label_position[:]-1.)*0.05

for list_SC in range(0,number_clusters):
	#Computing inverse of covariance matrix 
	Grid_nodes_covariances_M[:,:,:,:,:]     = Grid_nodes_covariances[:,:,:,:,:] + Grid_nodes_covariances_obs_large[list_SC,:,:,:,:,:]
	Grid_nodes_covariances_M_DET_inv[:,:,:] = np.linalg.det(Grid_nodes_covariances_M[:,:,:,:,:])**(-0.5)
	Grid_nodes_covariances_M_INV[:,:,:,:,:] = np.linalg.inv(Grid_nodes_covariances_M[:,:,:,:,:])

	observation = observations[list_SC,:]
	proba_node_3D = np.zeros((71,101,121))
	proba_node_age_mass = np.zeros((71,101))
	normalization = 1./(2*np.pi)**(0.5*components_number)
	normalization_here = np.zeros((71,101,10))
	normalization_here = normalization*Grid_nodes_covariances_M_DET_inv[:,:,:]*Grid_nodes_weights[:,:,:]  
	x_minus_mu = np.zeros((121,71,101,10,number_filters))
	dummy = np.zeros((121,71,101,10,number_filters))

	for ff in range(0,number_filters):
	 x_minus_mu[:,:,:,:,ff] = observation[ff] - Grid_nodes_means_reddened[:,:,:,:,ff]

	proba_node_3D, max_proba_indexes = probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_M_INV,Ext_min,Ext_max)

	#PEAK SOLUTION
	max_proba_indexes_bis = np.unravel_index(proba_node_3D.argmax(), proba_node_3D.shape)
	#aa = max_proba_indexes[0]
	#mm = max_proba_indexes[1]
	#Ebv = (max_proba_indexes[2]-1)*0.01
	aa = max_proba_indexes_bis[0]+1
	mm = max_proba_indexes_bis[1]+1
	zz=1
	Ebv = (max_proba_indexes_bis[2]+1-1)*0.01
	age, mass, Z, age_indice, mass_indice, Z_indice_bidon = Lecture_module.age_mass_Z_December2011(aa,mm,zz)
	print list_SC+1,age,np.log10(mass),Ebv
	solution[list_SC,0] = list_SC
	solution[list_SC,1] = age
	solution[list_SC,2] = np.log10(mass)
	solution[list_SC,3] = Ebv
	solution[list_SC,4] = proba_node_3D.max()

	'''
	#SOLUTION WITH SMALL AND LARGE SMOOTHING
	proba_node_3D_SmoothSmall = np.zeros((71,101,121))
	proba_node_3D_SmoothLarge = np.zeros((71,101,121))
	for mm_loop in range(1,72):
	 for Ext_loop in range(1,122):
          proba_node_3D_SmoothSmall[:,mm_loop-1,Ext_loop-1] = runningMeanFast(proba_node_3D[:,mm_loop-1,Ext_loop-1],3)
          proba_node_3D_SmoothLarge[:,mm_loop-1,Ext_loop-1] = runningMeanFast(proba_node_3D[:,mm_loop-1,Ext_loop-1],10)
	max_proba_indexes_SmoothSmall = np.unravel_index(proba_node_3D_SmoothSmall.argmax(), proba_node_3D_SmoothSmall.shape)
	max_proba_indexes_SmoothLarge = np.unravel_index(proba_node_3D_SmoothLarge.argmax(), proba_node_3D_SmoothLarge.shape)
	aa = max_proba_indexes_SmoothSmall[0]
	mm = max_proba_indexes_SmoothSmall[1]
	zz=1
	Ebv = (max_proba_indexes_SmoothSmall[2]-1)*0.01	
	age, mass, Z, age_indice, mass_indice, Z_indice = Lecture_module.age_mass_Z_December2011(aa,mm,zz)
	solution[list_SC,5] = age
	solution[list_SC,6] = np.log10(mass)
	solution[list_SC,7] = Ebv 

	aa = max_proba_indexes_SmoothLarge[0]
	mm = max_proba_indexes_SmoothLarge[1]
	zz=1
	Ebv = (max_proba_indexes_SmoothLarge[2]-1)*0.01	
	age, mass, Z, age_indice, mass_indice, Z_indice = Lecture_module.age_mass_Z_December2011(aa,mm,zz)
	solution[list_SC,8] = age
	solution[list_SC,9] = np.log10(mass)
	solution[list_SC,10] = Ebv
	'''

	#PICTURES: AGE vs MASS probability map
	#Marginalization on the extinction
	#proba_node_age_mass = np.zeros((71,71))
	#for mm in range(1,102):   
	# for aa in range(1,72):
	#  proba_node_age_mass[aa-1,mm-1] = proba_node_3D[aa-1,mm-1,:].sum()
	#np.save('/home/philippe/Desktop/fig_results/proba_node_age_mass_SC{0}.npy'.format(list_SC),proba_node_age_mass)

	'''
	fig=plt.figure()
	vector = np.linspace(1,71,71)
	hist,xedges,yedges = np.histogram2d(vector,vector, bins=(71,71), normed=False) #,range=[[6.575,10.125],[6.575,10.125]])
	extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]
	#plt.plot([6.6,10.1],[6.6,10.1], color='red', lw=1)
	res = plt.imshow(proba_node_age_mass[:,:].transpose(),interpolation='Nearest',aspect='auto',extent=extent,cmap=plt.cm.gist_yarg,origin='lower', norm=LogNorm(vmin=1e-150,vmax=np.max(proba_node_age_mass)))
	ylabel(r'$\log(m/M_{\odot})$')
	xlabel(r'$\log(t/\mathrm{yr})$')
	'''


	'''
	#1D CONVOLUTION:
	sigma_conv_age = 1
	proba_node_3D_smooth = Age_convolution_3D(proba_node_3D,sigma_conv_age)
	#1D CONVOLUTION MASS (after age!)
	sigma_conv_mass = 1
	proba_node_3D_smooth = Mass_convolution_3D(proba_node_3D_smooth,sigma_conv_mass)
	#1D CONVOLUTION EXTINCTION (after age and mass!)
	sigma_conv_Ebv = 1
	proba_node_3D_smooth = Ebv_convolution_3D(proba_node_3D_smooth,sigma_conv_Ebv)


	max_proba_indexes_Smooth = np.unravel_index(proba_node_3D_smooth.argmax(), proba_node_3D_smooth.shape)
	aa = max_proba_indexes_Smooth[0]+1
	mm = max_proba_indexes_Smooth[1]+1
	zz=1
	Ebv = (max_proba_indexes_Smooth[2]+1-1)*0.01	
	age, mass, Z, age_indice, mass_indice, Z_indice = Lecture_module.age_mass_Z_December2011(aa,mm,zz)
	solution[list_SC,5] = age
	solution[list_SC,6] = np.log10(mass)
	solution[list_SC,7] = Ebv 
	np.save('/home/philippe/Desktop/fig_results/proba_node_3D_SC{0}.npy'.format(list_SC),proba_node_3D)
	'''


	'''
	#Marginalization on the extinction
	proba_node_age_mass_smooth = np.zeros((71,71))
	for mm in range(1,72):   
	 for aa in range(1,72):
	  proba_node_age_mass_smooth[aa-1,mm-1] = proba_node_3D_smooth[aa-1,mm-1,:].sum()


	#PICTURES: AGE vs MASS probability map
	fig=plt.figure()
	vector = np.linspace(1,71,71)
	hist,xedges,yedges = np.histogram2d(vector-1,vector-1, bins=(71,71), normed=False) #,range=[[6.575,10.125],[6.575,10.125]])
	extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]
	res = plt.imshow(proba_node_age_mass_smooth[:,:].transpose(),interpolation='Nearest',
		         aspect='auto',extent=extent,cmap=plt.cm.gist_yarg,origin='lower',
	#                 norm=LogNorm(vmin=1e-40,vmax=np.max(proba_node_age_mass)))
	#                 norm=LogNorm(vmin=0.00000000001*np.max(proba_node_age_mass),vmax=np.max(proba_node_age_mass)))
	  	         norm=LogNorm(vmin=0.000001*np.max(proba_node_age_mass_smooth),vmax=np.max(proba_node_age_mass_smooth)))          
	ylabel(r'$\log(m/M_{\odot})$')
	xlabel(r'$\log(t/\mathrm{yr})$')
	levels = [0.5*np.max(proba_node_age_mass_smooth),0.1*np.max(proba_node_age_mass_smooth),0.01*np.max(proba_node_age_mass_smooth)]
	#print levels
	plt.contour(proba_node_age_mass_smooth.transpose(),levels,colors=('red','green','blue'))#,levels)
	plt.plot(np.unravel_index(proba_node_age_mass_smooth.argmax(), proba_node_age_mass_smooth.shape)[0],
		 np.unravel_index(proba_node_age_mass_smooth.argmax(), proba_node_age_mass_smooth.shape)[1],'ro',alpha=0.5)
	plt.plot(np.unravel_index(proba_node_age_mass.argmax(), proba_node_age_mass.shape)[0],
		 np.unravel_index(proba_node_age_mass.argmax(), proba_node_age_mass.shape)[1],'go',alpha=0.5)
	#filter_out_0 = ne.evaluate('proba_node_age_mass>0')
	#print np.min(proba_node_age_mass[filter_out_0]), np.max(proba_node_age_mass[filter_out_0])
	pylab.xticks(list_x_label_position, list_x_label_name)
	pylab.yticks(list_y_label_position, list_y_label_name)

	fig.savefig('/home/philippe/Desktop/fig_results/Age_Mass_SC{0}.png'.format(ID[list_SC]))
	close()
	'''

	'''
	#INTEGRATION AROUND EACH PIXEL OF PROBABILITY MAP. MAX OF INTEGRATION IS SOLUTION
	sigma_age=2
	sigma_mass=7
	sigma_Ebv=1
	proba_node_3D_integrated = Proba_map_integration(proba_node_3D,sigma_age,sigma_mass,sigma_Ebv)
	max_proba_indexes_Smooth = np.unravel_index(proba_node_3D_integrated.argmax(), proba_node_3D_integrated.shape)
	aa = max_proba_indexes_Smooth[0]+1
	mm = max_proba_indexes_Smooth[1]+1
	zz=1
	Ebv = (max_proba_indexes_Smooth[2]+1-1)*0.01	
	age, mass, Z, age_indice, mass_indice, Z_indice = Lecture_module.age_mass_Z_December2011(aa,mm,zz)
	solution[list_SC,5] = age
	solution[list_SC,6] = np.log10(mass)
	solution[list_SC,7] = Ebv 
	'''

	#IF WE WANT TO SAVE THE 3D PROBA MAPS IN NPY FORMAT (may be big)
	#np.save('/home/philippe/Desktop/npy_3D_PDF_from_analytic_FameClust_new/proba_node_3D_SC{0}_Z{1}.npy'.format(int(ID[list_SC]),sys.argv[4]),proba_node_3D)



#Output the solutions in file
file_out = open('/home/philippe/Desktop/All_clusters_parameters_results_f90_VANALYTIC_Z{0}_M700_FRS'.format((Z_indice)),'w')
print >> file_out,  '# ID  age3 mas3 Ebv3 Proba age_S mas_S Ebv_S'
np.savetxt(file_out,solution,('%5.0f','%.5f','%.5f','%.5f','%.7e','%.5f','%.5f','%.5f'))
file_out.close()
os.system("date")



