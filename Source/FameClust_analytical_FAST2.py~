
#This script is the first analytical test of FameClust method using GMM nodes

import numpy as np
import numexpr as ne
import scipy
import os
import math
import time
import random
from random import gauss
import pylab
from pylab import *
import sys
import Lecture_module  # (Module located in /home/philippe/Desktop/Discrete_models_comparaison_jtao)
import matplotlib.pyplot as plt
from sklearn import mixture 
from matplotlib.colors import LogNorm
from numba import jit
import f90_module_f2py  #module fortran imported throught F2PY
#from f90_module_f2py import blas_multiplication_gmm

#Simple 1 core use: time python FameClust_analytical_FAST2.py

def reddening(M1,A_lambda_filters_selected,Rv,Ebv):
 M2 = ne.evaluate("M1 + A_lambda_filters_selected * Rv*Ebv")
 return M2


'''
def probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV):
 f90_module_f2py.mod.comp_number = components_number
 f90_module_f2py.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((101,71,71,10,5))
 f90_module_f2py.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py.mod.proba_node_3D_all = np.zeros((71,71,101))		#proba_node_3D = np.zeros((71,71,101))
 for mm in range(1,72): 
  print mm
  for aa in range(1,72):

   for Ext in range(1,2):
	  Ebv = (Ext-1)*0.01
	  proba_component=np.zeros(components_number)
	  for ii in range(0,components_number): 

	   f90_module_f2py.mod.cov_inv = Grid_nodes_covariances_INV[aa-1,mm-1,ii,:,:]
	   f90_module_f2py.mod.x_mu = x_minus_mu[Ext-1,aa-1,mm-1,ii,:]
	   f90_module_f2py.mod.zero_matrix = np.zeros(5) #initial allocation
	   f90_module_f2py.mod.blas_multiplication_gmm()     #activation of the multiplication x_t*Cov_inv*x

	   proba_component[ii] = normalization_here[aa-1,mm-1,ii]*np.exp(-0.5*f90_module_f2py.mod.resultat)
	   proba_node_3D[aa-1,mm-1,Ext-1] = proba_node_3D[aa-1,mm-1,Ext-1] + proba_component[ii]
 return proba_node_3D
'''


def probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV):
 f90_module_f2py.mod.number_filter = 5
 f90_module_f2py.mod.comp_number = components_number
 f90_module_f2py.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((101,71,71,10,5))
 f90_module_f2py.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py.mod.proba_node_3d_all = np.zeros((71,71,101))		#proba_node_3D = np.zeros((71,71,101))

 f90_module_f2py.mod.blas_multiplication_gmm()

 #for mm in range(1,72): 
 # print mm
 # for aa in range(1,72):
 #
 #  for Ext in range(1,2):
 #	  Ebv = (Ext-1)*0.01
 #	  proba_component=np.zeros(components_number)
 #	  for ii in range(0,components_number): 
 #
 #	   f90_module_f2py.mod.cov_inv = Grid_nodes_covariances_INV[aa-1,mm-1,ii,:,:]
 #	   f90_module_f2py.mod.x_mu = x_minus_mu[Ext-1,aa-1,mm-1,ii,:]
 #	   f90_module_f2py.mod.zero_matrix = np.zeros(5) #initial allocation
 #	   f90_module_f2py.mod.blas_multiplication_gmm()     #activation of the multiplication x_t*Cov_inv*x
 #
 #	   proba_component[ii] = normalization_here[aa-1,mm-1,ii]*np.exp(-0.5*f90_module_f2py.mod.resultat)
 #	   proba_node_3D[aa-1,mm-1,Ext-1] = proba_node_3D[aa-1,mm-1,Ext-1] + proba_component[ii]

 return f90_module_f2py.mod.proba_node_3d_all, f90_module_f2py.mod.max_proba_position_gcc
'''

def probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV):
 number_filter = 5
 comp_number = components_number
 cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((101,71,71,10,5))
 normalization_here_all = normalization_here		#normalization_here = np.zeros((71,71,10))
 #proba_node_3d_all = np.zeros((71,71,101))		#proba_node_3D = np.zeros((71,71,101))

 proba_node_3D_all, max_proba_position_gcc = blas_multiplication_gmm(number_filter,comp_number,cov_inv_all,x_mu_all,normalization_here_all)

 return proba_node_3d_all, max_proba_position_gcc
'''






# -----------------------------------------------------------------------------------------
#Loading of the A_lambda (extinction parameters) for the filters selected in the input file
# -----------------------------------------------------------------------------------------
filters_selected_index = np.arange(5)
filters_selected_index[0] = 3 #FOR UBVRI
filters_selected_index[1] = 4 #FOR UBVRI
filters_selected_index[2] = 5 #FOR UBVRI
filters_selected_index[3] = 6 #FOR UBVRI
filters_selected_index[4] = 7 #FOR UBVRI
choice_extinction_law = 1
print
print 50*'-'
print 'Loading of the A_lambda (extinction parameters)'
print 50*'-'
print
# lambda, lambda_f_MW, lambda_f_LMC, index of filter [1-->52]
filters_A_lambda = np.genfromtxt('Filters_information.dat',comments='#')
if choice_extinction_law == 1:	#MW
 A_lambda_filters_selected = filters_A_lambda[filters_selected_index-1,1]
 Rv = 3.1
if choice_extinction_law == 2:	#LMC
 A_lambda_filters_selected = filters_A_lambda[filters_selected_index-1,2]
 Rv = 3.4
print A_lambda_filters_selected
print
print 50*'-'
print 'A_lambda loaded'
print 50*'-'
print
del filters_A_lambda
#raw_input()




#---------
#STEP ONE: 
#---------

os.system("date")
#index_parallel = int(sys.argv[1])
#print index_parallel
#raw_input()


#Metallicity of the grid
Z_indice = 'n00' #'m04'
Z,zz = Lecture_module.Zindex_to_Z_and_zz(Z_indice)

#Reading of observed clusters:
#No Ebv, no noise:
#file_obs = '/home/philippe/Desktop/Discrete_models_comparaison_jtao/SC_Parameters_20/test_1000_random_clusters/Test_1000_noise_noEbv/Fixed_mass/Half_Random_Sampling_results/UBVRI/FIXED_MASS_nonoise/UBVRI_Z01900_vs_other_Z_logm400/Clusters_created_M400_Zn00_Kroupa_ExpFactor6_completed'
#Ebv, noise:
file_obs = '/home/philippe/Desktop/Discrete_models_comparaison_jtao/SC_Parameters_20/test_1000_random_clusters/Test_1000_noise_Ebv/Fixed_mass/Half_Random_Sampling_results/UBVRI/UBVRI_Z01900_vs_other_Z_logm400/Clusters_created_M400_Zn00_Kroupa_ExpFactor6_completed'
data_obs = np.genfromtxt(file_obs,comments='#',names=True)
observations = np.zeros((10000,5))
age  = data_obs['Age']
mass = data_obs['Mass']
observations[:,0] = data_obs['U']
observations[:,1] = data_obs['B']
observations[:,2] = data_obs['V']
observations[:,3] = data_obs['R']
observations[:,4] = data_obs['I']

#observation = observations[1,:]


#READING THE GRID, AND STORING THE NODES
Grid_nodes_means           = np.zeros((71,71,10,5))
Grid_nodes_means_reddened  = np.zeros((101,71,71,10,5))
Grid_nodes_covariances     = np.zeros((71,71,10,5,5))
Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
Grid_nodes_covariances_DET_inv = np.zeros((71,71,10))
Grid_nodes_weights         = np.zeros((71,71,10))
Grid_nodes_covariances_obs = np.zeros((5,5)) #This is the cov matrix of uncertainties, containing sigmas of photometric errors
Grid_nodes_covariances_obs[0,0]=0.0025  #i.e. 0.05*0.05, variance, not stdev
Grid_nodes_covariances_obs[1,1]=0.0025
Grid_nodes_covariances_obs[2,2]=0.0025
Grid_nodes_covariances_obs[3,3]=0.0025 #9e59 for example in the case where we want to eliminate one filter. 
Grid_nodes_covariances_obs[4,4]=0.0025
print Grid_nodes_covariances_obs
Grid_nodes_covariances_M     = np.zeros((71,71,10,5,5))
Grid_nodes_covariances_M_INV = np.zeros((71,71,10,5,5))
Grid_nodes_covariances_M_DET = np.zeros((71,71,10))
Grid_nodes_covariances_M_DET_inv = np.zeros((71,71,10))
for mm in range(1,72):   
 print mm
 for aa in range(1,72):
  age, mass, Z, age_indice, mass_indice, Z_indice = Lecture_module.age_mass_Z_December2011(aa,mm,zz)

  #Open the npz node file   
  #path_grid = '/home/philippe/Desktop/Discrete_models_comparaison_jtao/Grid_HRS_allZ_ExpFactor6_Weidner_corrected_NPZ/NPZ_files/{0}/'.format(Z_indice) #in HDD
  path_grid = '/opt/Grid_HRS_allZ_ExpFactor6_Weidner_corrected_NPZ/NPZ_files/{0}/'.format(Z_indice) #in SSD
  node = np.load(path_grid+'Clusters_t{0}_M{1}_Z{2}.npz'.format(age_indice,mass_indice,Z_indice))

  #Extract the means, covariances and weights
  means       			  = node['means']
  covariances 			  = node['covariances']
  weights     			  = node['weights']
  Grid_nodes_weights[aa-1,mm-1,:] = node['weights']
  components_number = len(weights)

  #Save them in grids
  for ii in range(0,components_number):
   Grid_nodes_means[aa-1,mm-1,ii,:]           = means[ii,2:7]    
   Grid_nodes_covariances[aa-1,mm-1,ii,:,:]   = covariances[ii,2:7,2:7] 
   Grid_nodes_covariances_M[aa-1,mm-1,ii,:,:] = covariances[ii,2:7,2:7] + Grid_nodes_covariances_obs[:,:]

  for ii in range(0,components_number): 
   Grid_nodes_covariances_DET_inv[aa-1,mm-1,ii] = np.linalg.det(Grid_nodes_covariances[aa-1,mm-1,ii,:,:])**(-0.5)
   #normalization_here = normalization/np.linalg.det(covariances_UBVRI[ii])**0.5 
   Grid_nodes_covariances_obs_DET_inv           = np.linalg.det(Grid_nodes_covariances_obs[:,:])**(-0.5)
   Grid_nodes_covariances_M_DET[aa-1,mm-1,ii]   = np.linalg.det(Grid_nodes_covariances_M[aa-1,mm-1,ii,:,:])**0.5 
   Grid_nodes_covariances_M_DET_inv[aa-1,mm-1,ii]   = np.linalg.det(Grid_nodes_covariances_M[aa-1,mm-1,ii,:,:])**(-0.5) 

  for Ext in range(1,102):
   Ebv = (Ext-1)*0.01
   for ii in range(0,components_number):
    Grid_nodes_means_reddened[Ext-1,aa-1,mm-1,ii,:] = reddening(Grid_nodes_means[aa-1,mm-1,ii,:],A_lambda_filters_selected,Rv,Ebv)

  #Computing inverse of covariance matrix 
  Grid_nodes_covariances_INV[aa-1,mm-1,:,:,:]   = np.linalg.inv(Grid_nodes_covariances[aa-1,mm-1,:,:,:])
  Grid_nodes_covariances_M_INV[aa-1,mm-1,:,:,:] = np.linalg.inv(Grid_nodes_covariances_M[aa-1,mm-1,:,:,:])
  #a = covariances_UBVRI
  #ainv = np.linalg.inv(covariances_UBVRI) #We compute ALL the covariance matrices inverses in one shot!!
  #np.allclose(np.dot(a[1], ainv[1]), np.eye(5))      #Check that the inverse are well built

print 'GRID LOADED'
raw_input()

solution = np.zeros((1000,4))
for list_SC in range(0,101):
	observation = observations[list_SC,:]
	proba_node_3D = np.zeros((71,71,101))
	proba_node_age_mass = np.zeros((71,71))
	normalization = 1./(2*np.pi)**(0.5*components_number)
	normalization_here = np.zeros((71,71,10))
	#normalization_here = normalization*Grid_nodes_covariances_DET_inv[:,:,:]*Grid_nodes_weights[:,:,:]   		#for Sigma1
	normalization_here = normalization*Grid_nodes_covariances_M_DET_inv[:,:,:]*Grid_nodes_weights[:,:,:]   		#for Sigma1 modified --> M
	#normalization = (2*np.pi)**(0.5*components_number)
	#normalization_here = np.zeros((71,71,10))
	#normalization_here = normalization*Grid_nodes_covariances_DET_inv[:,:,:]*Grid_nodes_weights[:,:,:]  	#for Sigma1 
	#normalization_here[:,:,:] = normalization_here[:,:,:]*Grid_nodes_covariances_M_DET[:,:,:]   		#for M
	#normalization_here = normalization_here*Grid_nodes_covariances_obs_DET_inv   				#for Sigma2
	x_minus_mu = np.zeros((101,71,71,10,5))
	dummy = np.zeros((101,71,71,10,5))

	for ff in range(0,5):
	 x_minus_mu[:,:,:,:,ff] = observation[ff] - Grid_nodes_means_reddened[:,:,:,:,ff]

	#print x_minus_mu[:,:,:,:,ff]

	#proba_node_3D, max_proba_indexes = probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV)
	proba_node_3D, max_proba_indexes = probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_M_INV)
	#print max_proba_indexes
	aa = max_proba_indexes[0]
	mm = max_proba_indexes[1]
	zz=1
	Ebv = (max_proba_indexes[2]-1)*0.01
	age, mass, Z, age_indice, mass_indice, Z_indice = Lecture_module.age_mass_Z_December2011(aa,mm,zz)
	print list_SC+1,age,np.log10(mass),Ebv
	solution[list_SC,0] = list_SC
	solution[list_SC,1] = age
	solution[list_SC,2] = np.log10(mass)
	solution[list_SC,3] = Ebv

#Output the solutions in file
file_out = open('/home/philippe/Desktop/All_clusters_parameters_results_f90_VANALYTIC_Zn00','w')
print >> file_out,  '# ID  age3 mas3 Ebv3'
np.savetxt(file_out,solution)
file_out.close()

#Marginalization on the extinction
for mm in range(1,72):   
 for aa in range(1,72):
  proba_node_age_mass[aa-1,mm-1] = proba_node_3D[aa-1,mm-1,:].sum()



plt.figure()
vector = np.linspace(1,71,71)
hist,xedges,yedges = np.histogram2d(vector,vector, bins=(71,71), normed=False) #,range=[[6.575,10.125],[6.575,10.125]])
extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]
#plt.plot([6.6,10.1],[6.6,10.1], color='red', lw=1)
res = plt.imshow(proba_node_age_mass[:,:].transpose(),interpolation='Nearest',aspect='auto',extent=extent,cmap=plt.cm.gist_yarg,origin='lower', norm=LogNorm(vmin=1e-150,vmax=np.max(proba_node_age_mass)))

ylabel(r'$\log(m/M_{\odot})$')
xlabel(r'$\log(t/\mathrm{yr})$')
plt.show()


#  '''







