
#This script is the first analytical test of FameClust method using GMM nodes

import numpy as np
import numexpr as ne
import scipy
from scipy import signal
import os
import math
import time
import random
from random import gauss
import pylab
from pylab import *
import sys
import Lecture_module  # (Module located in /home/philippe/Desktop/Discrete_models_comparaison_jtao)
import matplotlib.pyplot as plt
#from sklearn import mixture 
from matplotlib.colors import LogNorm
#from numba import jit
#import f90_module_f2py_GALEX_UBVRI
#import f90_module_f2py_UBVRI  #module fortran imported throught F2PY
import f90_module_f2py_WFC3  #module fortran imported throught F2PY
#import f90_module_f2py_UBVRIJHK  #module fortran imported throught F2PY
#from f90_module_f2py import blas_multiplication_gmm

#Simple 1 core use: time python FameClust_analytical_FAST5.py InputFameClustNEW_WFC3_PHAT 1 601 n00
#or: time python FameClust_analytical_FAST5.py InputFameClustNEW_WFC3_Z01900_M400_V105 1 10 n00

# ------------------------
# Declaration of functions
# ------------------------
def reddening(M1,A_lambda_filters_selected,Rv,Ebv):
 M2 = ne.evaluate("M1 + A_lambda_filters_selected * Rv*Ebv")
 return M2


def probability_building_WFC3(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV,Ext_min,Ext_max):
 #f90_module_f2py_WFC3.mod.E_mini = Ext_min
 #f90_module_f2py_WFC3.mod.E_maxi = Ext_max
 f90_module_f2py_WFC3.mod.bidu1 = Ext_min
 f90_module_f2py_WFC3.mod.bidu2 = Ext_max
 f90_module_f2py_WFC3.mod.comp_number = components_number
 f90_module_f2py_WFC3.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py_WFC3.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((121,71,71,10,5))
 f90_module_f2py_WFC3.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py_WFC3.mod.proba_node_3d_all = np.zeros((71,101,121))		#proba_node_3D = np.zeros((71,71,121))
 f90_module_f2py_WFC3.mod.blas_multiplication_gmm()
 return f90_module_f2py_WFC3.mod.proba_node_3d_all, f90_module_f2py_WFC3.mod.max_proba_position_gcc


def probability_building_UBVRI(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV,Ext_min,Ext_max):
 #f90_module_f2py_UBVRI.mod.Ext_minimum = 7 #Ext_min
 #f90_module_f2py_UBVRI.mod.Ext_maximum = 121 #Ext_max
 f90_module_f2py_UBVRI.mod.bidu1 = Ext_min
 f90_module_f2py_UBVRI.mod.bidu2 = Ext_max
 f90_module_f2py_UBVRI.mod.comp_number = components_number
 f90_module_f2py_UBVRI.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py_UBVRI.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((121,71,71,10,5))
 f90_module_f2py_UBVRI.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py_UBVRI.mod.proba_node_3d_all = np.zeros((71,101,121))		#proba_node_3D = np.zeros((71,71,121))
 f90_module_f2py_UBVRI.mod.blas_multiplication_gmm()
 return f90_module_f2py_UBVRI.mod.proba_node_3d_all, f90_module_f2py_UBVRI.mod.max_proba_position_gcc

'''
def probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV):
 f90_module_f2py_UBVRI.mod.comp_number = components_number
 f90_module_f2py_UBVRI.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py_UBVRI.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((121,71,71,10,5))
 f90_module_f2py_UBVRI.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py_UBVRI.mod.proba_node_3d_all = np.zeros((71,71,121))		#proba_node_3D = np.zeros((71,71,121))
 f90_module_f2py_UBVRI.mod.blas_multiplication_gmm()
 return f90_module_f2py_UBVRI.mod.proba_node_3d_all, f90_module_f2py_UBVRI.mod.max_proba_position_gcc

def probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV):
 f90_module_f2py_UBVRIJHK.mod.comp_number = components_number
 f90_module_f2py_UBVRIJHK.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py_UBVRIJHK.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((121,71,71,10,5))
 f90_module_f2py_UBVRIJHK.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py_UBVRIJHK.mod.proba_node_3d_all = np.zeros((71,71,121))		#proba_node_3D = np.zeros((71,71,121))
 f90_module_f2py_UBVRIJHK.mod.blas_multiplication_gmm()
 return f90_module_f2py_UBVRIJHK.mod.proba_node_3d_all, f90_module_f2py_UBVRIJHK.mod.max_proba_position_gcc

def probability_building(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV):
 f90_module_f2py_GALEX_UBVRI.mod.comp_number = components_number
 f90_module_f2py_GALEX_UBVRI.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py_GALEX_UBVRI.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((121,71,71,10,5))
 f90_module_f2py_GALEX_UBVRI.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py_GALEX_UBVRI.mod.proba_node_3d_all = np.zeros((71,71,121))		#proba_node_3D = np.zeros((71,71,121))
 f90_module_f2py_GALEX_UBVRI.mod.blas_multiplication_gmm()
 return f90_module_f2py_GALEX_UBVRI.mod.proba_node_3d_all, f90_module_f2py_GALEX_UBVRI.mod.max_proba_position_gcc
'''




 


#Here is the tool which allows to derive the correct A_lambda_on_Av for the desired E(B-V),age, and filters (of SSP Zn00)


'''
#Parameters
age_selected = 7.1 
Ebv_selected = 3.1
number_filters = 6
filters_selected_index = np.arange(number_filters)
filters_selected_index[0] = 11
filters_selected_index[1] = 12
filters_selected_index[2] = 13
filters_selected_index[3] = 14
filters_selected_index[4] = 15
filters_selected_index[5] = 16
'''
def Find_correct_A_lambda_on_Av(A_lambda_on_Av_all_Av,age_selected,Ebv_selected,filters_selected_index):
 Av = 3.1*Ebv_selected
 low_Av = np.int(np.floor(Av))
 up_Av  = np.int(np.ceil(Av))    
 Av_array_low_up = np.zeros(2)
 Av_array_low_up[0] = np.floor(Av)
 Av_array_low_up[1] = np.ceil(Av)
 #print Av_array_low_up, Av     
 number_filters = len(filters_selected_index)       
 #Here we select the Av for the correct age and filters 
 age_array = A_lambda_on_Av_all_Av[:,0,0]
 index_age = ne.evaluate('(age_array>=age_selected-0.001) & (age_array<=age_selected+0.001)') #Selection of good age   
 Av_array_low_up_value = np.zeros((2,number_filters))
 Av_array_low_up_value[0,:] = A_lambda_on_Av_all_Av[index_age,filters_selected_index[0]:filters_selected_index[0]+number_filters,low_Av]
 Av_array_low_up_value[1,:] = A_lambda_on_Av_all_Av[index_age,filters_selected_index[0]:filters_selected_index[0]+number_filters,up_Av]
 #print A_lambda_on_Av_all_Av[index_age,11:17,low_Av]
 #print A_lambda_on_Av_all_Av[index_age,11:17,up_Av]
 #Here we interpolate between the Av tables to get the correct values for selected Av        
 A_lambda_Av_used = np.zeros(number_filters)
 for ff in range(0,number_filters):
  A_lambda_Av_used[ff] = np.interp(Av,Av_array_low_up,Av_array_low_up_value[:,ff])    
 return A_lambda_Av_used
'''
A_lambda_Av_used = Find_correct_A_lambda_on_Av(A_lambda_on_Av_all_Av,age_selected,Ebv_selected,filters_selected_index)
print A_lambda_Av_used
'''





# -------------------------
# Loading of the input file
# -------------------------

print
print 80*'-'
print
print 50*'-'
print 'Loading of InputFile'
print 50*'-'
print


os.system("date")
#InputFile_Name = sys.argv[1] # The name of input file is given during execution of the script
#number_begin = int(sys.argv[2])
#number_end = int(sys.argv[3])
number_clusters = int(sys.argv[3])
#Z_indice = sys.argv[4]
#Z,zz = Lecture_module.Zindex_to_Z_and_zz(Z_indice)


#Reading of the command-line arguments
InputFile_Name = sys.argv[1]    # The name of input file is given during execution of the script
number_begin = int(sys.argv[2]) 
number_end = int(sys.argv[3])   
Z_indice_selected = sys.argv[4]

#Determination of the metallicity in the different needed formats: Z, [M/H] and zz (index)
Z_selected, zz = Lecture_module.Zindex_to_Z_and_zz(Z_indice_selected)
print Z_indice_selected, Z_selected, zz

#Input file is read in the function Reading_of_InputFile
number_filters        ,       \
filters_selected_index,       \
Distance_modulus_host_galaxy, \
app_or_abs,                   \
file_observed_clusters,       \
number_cluster_observed,      \
choice_extinction,            \
choice_extinction_law,        \
path_file_out_cluster,        \
Grid_path1,                   \
Grid_path2,                   \
number_nodes_age,             \
number_nodes_mass,            \
number_models_per_node,       \
number_of_filters_in_grid,    \
File_information_filters_and_ExtCurve, \
Min_extinction,               \
Max_extinction,               \
choice_RealClusters_or_ArtificialTest = Lecture_module.Reading_of_InputFile(InputFile_Name)

print
print 50*'-'
print 'InputFile loaded'
print 50*'-'
print



# -----------------------------------------------------------------------------------------
#Loading of the A_lambda (extinction parameters) for the filters selected in the input file
# -----------------------------------------------------------------------------------------
#PART 1: we load the fixed A_lambda/Av
choice_extinction_law = 1
print
print 50*'-'
print 'Loading of the A_lambda (extinction parameters)'
print 50*'-'
# lambda, lambda_f_MW, lambda_f_LMC, index of filter [1-->52]
filters_A_lambda = np.genfromtxt('Filters_information.dat',comments='#')
#filters_A_lambda = np.genfromtxt('Filters_information_observations_WFC3_from_Welch.dat',comments='#')
#filters_A_lambda = np.genfromtxt('Filters_information_observations_WFC3_from_STSCI.dat',comments='#')
if choice_extinction_law == 1:	#MW
 A_lambda_filters_selected = filters_A_lambda[filters_selected_index-1,1]
 Rv = 3.1
if choice_extinction_law == 2:	#LMC
 A_lambda_filters_selected = filters_A_lambda[filters_selected_index-1,2]
 Rv = 3.4
print A_lambda_filters_selected
del filters_A_lambda

#PART 2: We load all the A_lambda/Av tables in one 3D array (age,filters,Av); this is a more accurate description of the extinction
A_lambda_on_Av_all_Av = np.zeros((71,1+52,11))
for Av_index in range(0,11):
 A_lambda_on_Av_all_Av[:,:,Av_index] = np.genfromtxt('/home/philippe/Desktop/Discrete_models_comparaison_jtao/A_lambda_on_Av_grid_Zn00/A_lambda_on_Av_for_Av_{0}'.format(Av_index),comments='#')

print 50*'-'
print 'A_lambda loaded'
print 50*'-'
print
#raw_input()


# -----------------------------------------------------------------------------------------
#Loading of the observations
# -----------------------------------------------------------------------------------------
choice_sigma_observation = 0 #Artificial tests
#choice_sigma_observation = 1 #data with sigmas given
print
print 50*'-'
print 'Loading of the observation'
print 50*'-'
data_input         = np.genfromtxt(path_file_out_cluster+file_observed_clusters, comments='#')
data_input_names   = np.genfromtxt(path_file_out_cluster+file_observed_clusters, comments='#',names=True)
ID = data_input_names['ID']
observations       = np.zeros((len(data_input[:,0]),number_filters))
observations_sigma = np.zeros((len(data_input[:,0]),number_filters))
print 'Filters loaded: '
for ff in range(0,number_filters):
 name_filter, e_name_filter = Lecture_module.Filters_Index_Name(filters_selected_index[ff])
 print name_filter
 observations[:,ff]       = data_input_names[name_filter]
 if choice_sigma_observation == 1: #data with given sigmas
  observations_sigma[:,ff] = np.maximum(0.05,data_input_names[e_name_filter])
 # observations_sigma[:,ff] = data_input_names[e_name_filter]
 if choice_sigma_observation == 0: #Artificial tests with artificial sigmas
  observations_sigma[:,ff] = 0.05  #MODIFY THIS
#observations_sigma[:,5:8] = 0.10
#observations_sigma[:,0:2] = 0.15 #9e59
print 'observations: ', observations
observations[:,:] = observations[:,:] - np.float(Distance_modulus_host_galaxy)
#REJECTING GAP DATA (by increasing the sigmas of these gaps data to infinite)
observations_gaps_mask = ne.evaluate('observations>30.')
observations[observations_gaps_mask] = 99.999
observations_sigma[observations_gaps_mask] = 9e19
print 'observations: ', observations

#We filter out the sigmas of bad data: from 99.9 -> 9e59   #this works when choice_sigma_observation == 1 (when using real sigmas)
filter_sigma = ne.evaluate('observations_sigma>10.')       #this filter is based on rejecting data with very large sigmas
observations_sigma[filter_sigma] = 9e19

#observations_sigma[:,0] = observations_sigma[:,0] + 0.03
#observations_sigma[:,1] = observations_sigma[:,1] + 0.03
#observations_sigma[:,2] = observations_sigma[:,2] + 0.03
#observations_sigma[:,3] = observations_sigma[:,3] + 0.05
#observations_sigma[:,4] = observations_sigma[:,4] + 0.20
#observations_sigma[:,5] = observations_sigma[:,5] + 0.30

print 'sigmas of observations: ', observations_sigma
#age_true  = data_input_names['logt']	#If available data
#mass_true = data_input_names['logm']	#If available data
#Ebv_true  = data_input_names['Av']*3.1	#If available data
del data_input,data_input_names
print 50*'-'
print 'Observation loaded'
print 50*'-'
print
#raw_input()

Ebv_min = Min_extinction #0.06
Ebv_max = Max_extinction #1.1
Ext_min,Ext_max=np.int(Ebv_min*100+1),np.int(Ebv_max*100+1)
#print Ext_min,Ext_max


#READING THE GRID, AND STORING THE NODES
Grid_nodes_means                   = np.zeros((71,101,10,number_filters))
Grid_nodes_means_reddened          = np.zeros((121,71,101,10,number_filters))
Grid_nodes_covariances             = np.zeros((71,101,10,number_filters,number_filters))
Grid_nodes_covariances_INV         = np.zeros((71,101,10,number_filters,number_filters))
Grid_nodes_covariances_DET_inv     = np.zeros((71,101,10))
Grid_nodes_weights                 = np.zeros((71,101,10))
Grid_nodes_covariances_M           = np.zeros((71,101,10,number_filters,number_filters))
Grid_nodes_covariances_M_INV       = np.zeros((71,101,10,number_filters,number_filters))
Grid_nodes_covariances_M_DET       = np.zeros((71,101,10))
Grid_nodes_covariances_M_DET_inv   = np.zeros((71,101,10))

for mm in range(1,102):   
 print mm
 for aa in range(1,72):
  age, mass, Z, age_indice, mass_indice, Z_indice = Lecture_module.age_mass_Z_December2011(aa,mm,zz)

  #Open the npz node file   
  #path_grid = '/home/philippe/Desktop/Discrete_models_comparaison_jtao/Grid_HRS_allZ_ExpFactor6_Weidner_corrected_NPZ/NPZ_files/{0}/'.format(Z_indice) #in HDD
  #path_grid = '/opt/Grid_HRS_allZ_ExpFactor6_Weidner_corrected_NPZ/NPZ_files/{0}/'.format(Z_indice) #in SSD    (1000 models per node)  
  #path_grid = '/opt/Grid_HRS_allZ_ExpFactor6_Weidner_corrected_NPZ/NPZ_files_5000models_per_node/{0}/'.format(Z_indice) #in SSD    (1000 models per node)  
  #path_grid = '/home/philippe/Desktop/Discrete_models_comparaison_jtao/Grid_HRS_allZ_ExpFactor6_Weidner_corrected_NPZ/NPZ_files_5000models_per_node_with_ACS_PEGASE/{0}/'.format(Z_indice) #in HDD    (1000 models per node)  #CENTERED ON PEGASE SSP!!!!
  #path_grid = '/home/philippe/Desktop/Discrete_models_comparaison_jtao/Grid_FRS_allZ_Kroupa_NPZ/NPZ_files/{0}/'.format(Z_indice) #in HDD    (1000 models per node)  #CENTERED ON PADOVA SSP
  #path_grid = '/home/philippe/Desktop/Discrete_models_comparaison_jtao/Grid_FRS_allZ_Kroupa_NPZ/NPZ_files_PEGASE/{0}/'.format(Z_indice) #in HDD    (1000 models per node)  #CENTERED ON PEGASE SSP!!!!

  #path_grid = '/opt/Grid_FRS_allZ_Kroupa_NPZ/NPZ_files/{0}/'.format(Z_indice) #in SSD    #CENTERED ON PADOVA SSP

  #DENSE GRID
  path_grid = '/home/philippe/Desktop/Discrete_models_comparaison_jtao/Grids_with_ACS/Grid_FRS_DENSE_AgeInterpolated/NPZ_files/{0}/'.format(Z_indice) #in SSD    #CENTERED ON PADOVA SSP

  node = np.load(path_grid+'Clusters_t{0}_M{1}_Z{2}.npz'.format(age_indice,mass_indice,Z_indice))

  #Extract the means, covariances and weights
  means       			  = node['means']
  covariances 			  = node['covariances']
  weights     			  = node['weights']
  Grid_nodes_weights[aa-1,mm-1,:] = node['weights']
  components_number = len(weights)

  #Extracting them for the good filters, in grids
  for ii in range(0,components_number):
   Grid_nodes_means[aa-1,mm-1,ii,:]           = means[ii,filters_selected_index[0]-1:filters_selected_index[0]-1+number_filters]    
   Grid_nodes_covariances[aa-1,mm-1,ii,:,:]   = covariances[ii,filters_selected_index[0]-1:filters_selected_index[0]-1+number_filters,filters_selected_index[0]-1:filters_selected_index[0]-1+number_filters] 

  #REDDENING OF THE MEANS, TO ACCOUNT FOR EXTINCTED NODES
  for Ext in range(Ext_min,Ext_max+1):  #1,122 -> E(B-V)=[0,1]
   Ebv = (Ext-1)*0.01

   #HERE I INCLUDE THE SOPHISTICATION ON EXTINCTION (this is an option, comment it if not desired. 
   #If commented, value will be from the file opened in #Loading of the A_lambda#, it will be fixed values correct for Vega star only )
   #A_lambda_filters_selected = Find_correct_A_lambda_on_Av(A_lambda_on_Av_all_Av,age,Ebv,filters_selected_index)
   #print A_lambda_filters_selected
   #A_lambda_filters_selected[0] = A_lambda_filters_selected[0] - 0.002
   #A_lambda_filters_selected[1] = A_lambda_filters_selected[1] - 0.003
   #A_lambda_filters_selected[2] = A_lambda_filters_selected[2] - 0.02 
   #A_lambda_filters_selected[3] = A_lambda_filters_selected[3] - 0.015
   #A_lambda_filters_selected[4] = A_lambda_filters_selected[4] - 0.018
   #A_lambda_filters_selected[5] = A_lambda_filters_selected[5] - 0.0035

   for ii in range(0,components_number):
    Grid_nodes_means_reddened[Ext-1,aa-1,mm-1,ii,:] = reddening(Grid_nodes_means[aa-1,mm-1,ii,:],A_lambda_filters_selected,Rv,Ebv)
print 'GRID LOADED'
print 'from: ', path_grid
#print A_lambda_filters_selected
#raw_input()


#BUILDING OF THE MATRIX CONTAINING THE SIGMAS OF DATA
Grid_nodes_covariances_obs_large   = np.zeros((number_clusters,71,101,10,number_filters,number_filters))
Grid_nodes_covariances_obs         = np.zeros((number_filters,number_filters)) #This is the cov matrix of uncertainties, containing sigmas of photometric errors

#if choice_sigma_observation == 0: #For the artificial tests  !HERE BUG!!! BE CAREFUL IF WE USE AUTOMATIC SIGMAS FOR REAL DATA, WHICH HAVE GAPS!!
# for ff in range(0,number_filters): #Building the sigma covariance matrix
#  Grid_nodes_covariances_obs[ff,ff] = observations_sigma[0,ff]*observations_sigma[0,ff]  #variance, not stdev

#here we build the large matrix (containing the sigmas for all the clusters)
for list_SC in range(0,number_clusters):
 print list_SC
 #if choice_sigma_observation == 1: #For data with given sigmas
 for ff in range(0,number_filters): #Building the sigma covariance matrix
  Grid_nodes_covariances_obs[ff,ff] = observations_sigma[list_SC,ff]*observations_sigma[list_SC,ff]  #variance, not stdev
 if list_SC==0: print Grid_nodes_covariances_obs
 for mm in range(1,102):   
  for aa in range(1,72):
   for ii in range(0,components_number):
    Grid_nodes_covariances_obs_large[list_SC,aa-1,mm-1,ii,:,:] = Grid_nodes_covariances_obs[:,:]
print 'MATRIX SIGMA LOADED'  #NOTE THAT THIS TRIPLE LOOP HERE NEEDS A LOT OF RAM. I SHOULD SIMPLIFY!!!
#raw_input()
os.system("date")




'''
def reddening(M1,A_lambda_filters_selected,Rv,Ebv):
 M2 = ne.evaluate("M1 + A_lambda_filters_selected * Rv*Ebv")
 return M2


def probability_building_WFC3_14March2016(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_INV,Ext_min,Ext_max):
 f90_module_f2py_WFC3.mod.Ext_minimum = Ext_min
 f90_module_f2py_WFC3.mod.Ext_maximum = Ext_max
 f90_module_f2py_WFC3.mod.bidu1 = Ext_min
 f90_module_f2py_WFC3.mod.bidu2 = Ext_max
 f90_module_f2py_WFC3.mod.comp_number = components_number
 f90_module_f2py_WFC3.mod.cov_inv_all = Grid_nodes_covariances_INV 		#[:,:,:,:,:]  Grid_nodes_covariances_INV = np.zeros((71,71,10,5,5))
 f90_module_f2py_WFC3.mod.x_mu_all    = x_minus_mu 				#x_minus_mu = np.zeros((121,71,71,10,5))
 f90_module_f2py_WFC3.mod.normalization_here_all = normalization_here	#normalization_here = np.zeros((71,71,10))
 f90_module_f2py_WFC3.mod.proba_node_3d_all = np.zeros((71,101,121))		#proba_node_3D = np.zeros((71,71,121))
 f90_module_f2py_WFC3.mod.blas_multiplication_gmm()
 return f90_module_f2py_WFC3.mod.proba_node_3d_all, f90_module_f2py_WFC3.mod.max_proba_position_gcc
'''

os.system("date")
raw_input()


solution = np.zeros((number_clusters,8))


#THE MAIN LOOP
for list_SC in range(0,number_clusters):

	#Computing the general (merged) covariance matrix 
	Grid_nodes_covariances_M[:,:,:,:,:]     = Grid_nodes_covariances[:,:,:,:,:] + Grid_nodes_covariances_obs_large[list_SC,:,:,:,:,:]

	#Computing determinant of covariance matrix (and inversing it after, as suitable for the following)
	Grid_nodes_covariances_M_DET_inv[:,:,:] = np.linalg.det(Grid_nodes_covariances_M[:,:,:,:,:])**(-0.5)

	#print Grid_nodes_covariances_M_DET_inv[0,0,0],Grid_nodes_covariances_M_DET_inv[0,0,4]
	#print Grid_nodes_covariances_M_DET_inv[29,29,0],Grid_nodes_covariances_M_DET_inv[29,29,4]
	#print Grid_nodes_covariances_M_DET_inv[49,49,0],Grid_nodes_covariances_M_DET_inv[49,49,4]


	#Computing inverse of covariance matrix 
	Grid_nodes_covariances_M_INV[:,:,:,:,:] = np.linalg.inv(Grid_nodes_covariances_M[:,:,:,:,:])

	#print Grid_nodes_covariances_M_INV[0,0,0,0,0],Grid_nodes_covariances_M_INV[0,0,4,0,0]
	#print Grid_nodes_covariances_M_INV[29,29,0,0,0],Grid_nodes_covariances_M_INV[29,29,4,0,0]
	#print Grid_nodes_covariances_M_INV[49,49,0,0,0],Grid_nodes_covariances_M_INV[49,49,4,0,0]


	observation = observations[list_SC,:]
	proba_node_3D = np.zeros((71,101,121))
	#proba_node_age_mass = np.zeros((71,101))
	normalization = 1./(2*np.pi)**(0.5*components_number)
	normalization_here = np.zeros((71,101,10))
	normalization_here = normalization*Grid_nodes_covariances_M_DET_inv[:,:,:]*Grid_nodes_weights[:,:,:]  
	x_minus_mu = np.zeros((121,71,101,10,number_filters))
	dummy = np.zeros((121,71,101,10,number_filters))

	for ff in range(0,number_filters):
	 x_minus_mu[:,:,:,:,ff] = observation[ff] - Grid_nodes_means_reddened[:,:,:,:,ff]

	proba_node_3D, max_proba_indexes = probability_building_WFC3(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_M_INV,Ext_min,Ext_max)

	#proba_node_3D, max_proba_indexes = probability_building_WFC3_14March2016(components_number,dummy,normalization_here,x_minus_mu,Grid_nodes_covariances_M_INV,Ext_min,Ext_max)

	#PEAK SOLUTION
	aa = max_proba_indexes[0]
	mm = max_proba_indexes[1]
	Ebv = (max_proba_indexes[2]-1)*0.01
	#max_proba_indexes_bis = np.unravel_index(proba_node_3D.argmax(), proba_node_3D.shape)
	#aa = max_proba_indexes_bis[0]+1
	#mm = max_proba_indexes_bis[1]+1
	zz=1
	#Ebv = (max_proba_indexes_bis[2]+1-1)*0.01
	age, mass, Z, age_indice, mass_indice, Z_indice_bidon = Lecture_module.age_mass_Z_December2011(aa,mm,zz)
	print list_SC+1,age,np.log10(mass),Ebv
        #print aa,mm,Ebv, max_proba_indexes[0], max_proba_indexes[1],(max_proba_indexes[2]-1)*0.01  
	solution[list_SC,0] = list_SC
	solution[list_SC,1] = age
	solution[list_SC,2] = np.log10(mass)
	solution[list_SC,3] = Ebv
	solution[list_SC,4] = proba_node_3D.max()



	#IF WE WANT TO SAVE THE 3D PROBA MAPS IN NPY FORMAT (may be big)
	#np.save('/home/philippe/Desktop/npy_3D_PDF_from_analytic_FameClust_new/proba_node_3D_SC{0}_Z{1}.npy'.format(int(ID[list_SC]),sys.argv[4]),proba_node_3D)



#Output the solutions in file
file_out = open(path_file_out_cluster+'All_clusters_parameters_results_f90_VANALYTIC_Z{0}_M700_FRS'.format((Z_indice)),'w')
print >> file_out,  '# ID  age3 mas3 Ebv3 Proba age_S mas_S Ebv_S'
np.savetxt(file_out,solution,('%5.0f','%.5f','%.5f','%.5f','%.7e','%.5f','%.5f','%.5f'))
file_out.close()
os.system("date")





















# DEPRECATED PIECES OF CODE


'''
# -------------------------
# Loading of the input file
# -------------------------
print
print 80*'-'
print
print 50*'-'
print 'Loading of InputFile'
print 50*'-'

os.system("date")
InputFile_Name = sys.argv[1] # The name of input file is given during execution of the script
number_begin = int(sys.argv[2])
number_end = int(sys.argv[3])
number_clusters = int(sys.argv[3])
Z_indice = sys.argv[4]
Z,zz = Lecture_module.Zindex_to_Z_and_zz(Z_indice)


InputFile = open('/home/philippe/Desktop/Discrete_models_comparaison_jtao/SC_Parameters_20/'+InputFile_Name).readlines()
my_list = []
for line in InputFile:
    item = str.split(line)
    if item[0][0] != '#':
     my_list.append(item[0])

number_filters = int(my_list[0])
print 'Number of filters selected:                 ', number_filters

filters_selected_index = np.arange(number_filters) 	#integer array containing the indexes of the selected filters 
for ii in range(0,number_filters):
 filters_selected_index[ii] = int(my_list[ii+1])
print 'Indexes of filters selected:               ', filters_selected_index

Distance_modulus_host_galaxy = my_list[number_filters+1]
print 'Distance modulus of the host galaxy:        ', Distance_modulus_host_galaxy
app_or_abs = int(my_list[number_filters+2])
print 'Apparent mags [1], Absolute mags [2]:       ', app_or_abs
file_observed_clusters = my_list[number_filters+3]
print 'Input file of the observed clusters:        '
print '    ',file_observed_clusters
number_cluster_observed = int(my_list[number_filters+4])
print 'Number of observed clusters:                ',  number_cluster_observed	#[obsolete]
choice_extinction = int(my_list[number_filters+5])
print 'Cluster(s) extincted [1], not extincted [2]:', choice_extinction
choice_extinction_law = int(my_list[number_filters+6])
print 'Extinction law of MW [1], of LMC [2]:       ', choice_extinction_law
path_file_out_cluster = my_list[number_filters+7]
print 'Path of output files for derived parameters:' 
print '    ',path_file_out_cluster
print 50*'-'
print 'InputFile loaded'
print 50*'-'
print
#raw_input()
'''





